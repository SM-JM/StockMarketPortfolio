# -*- coding: utf-8 -*-
"""TwitterDataExtraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MBW9xG2gLOEfc09vPJM3pyzF_IT0x9It

# Preliminary Items

## Imports
"""

!pip install searchtweets

import os
import pandas as pd

import requests
import json

bearer_token = ''

"""Enable colab pandas interactive displays"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import data_table
# %load_ext google.colab.data_table

"""## Mount Google Drive """

# Commented out IPython magic to ensure Python compatibility.
currentWorkingDir = !pwd
defaultWorkingDir = "/content"

if ( currentWorkingDir[0] == defaultWorkingDir ):
  from google.colab import drive

  drive.mount('/content/drive')
      
#   %cd "/content/drive/My Drive/Colab Notebooks/stock_portfolio/"
else:
  print("Currenting running app from: ")
  !pwd

"""# Data Understanding

## Set Search URL and Query
"""

search_url = "https://api.twitter.com/1.1/tweets/search/fullarchive/Production.json"

# Tweets extracted already
#'("$CCC.ja" OR "Carib Cement" -"JSEInvestor.com") lang:en toDate:202012310000 fromDate:201601010000'
# ("$JBG.ja" OR "JAMAICA BROILERS" -"JSEInvestor.com") lang:en until:2020-12-31 since:2016-01-01
# ("$KW.ja" OR "KINGSTON WHARVES" -"JSEInvestor.com") lang:en until:2020-12-31 since:2016-01-01

symbol = "jbg"
symbol_full_name = "Jamaica Broilers"
continue_previous_extract = False
continue_number = 0

# Query
query_params = {
                  'query'       : '("${}.ja" OR "{}" OR "{} jamaica" -"RT" -"JSEInvestor.com" -is:retweet lang:en )'.format(symbol,symbol_full_name,symbol), 
                  'maxResults'  : "500",
                  'fromDate'    : '201601010000',
                  'toDate'      : '202101010000'
                }

if (continue_previous_extract):
  query_params['next'] = ""

"""## Twitter API Functions"""

def create_headers(bearer_token):
    headers = {"Authorization": "Bearer {}".format(bearer_token)}
    return headers


def connect_to_endpoint(url, headers, params):
    response = requests.request("GET", search_url, headers=headers, params=params)
    print(response.status_code)
    if response.status_code != 200:
        raise Exception(response.status_code, response.text)
    return response.json()

"""## Extract Tweets and save output to JSON files"""

num = 0 #count for different tweets

if continue_previous_extract:
  num = continue_number


headers = create_headers(bearer_token)
json_response = connect_to_endpoint(search_url, headers, query_params)

# If there are 500 or less total tweets from query, extract then save it to file
if not( 'next' in json_response ):
    with open('tweets_'+symbol+str(num)+'.json', 'w') as outfile:
      outfile.write(json.dumps(json_response, indent=4, sort_keys=True))

# Otherwise, do a loop to automatically save tweets
else:
  # Create a loop to automatically save 

  with open('tweets_'+symbol+str(num)+'.json', 'w') as outfile:
    outfile.write(json.dumps(json_response, indent=4, sort_keys=True))

  query_params['next'] = json_response['next']

  num += 1

  while ( 'next' in json_response ):

    json_response = connect_to_endpoint(search_url, headers, query_params)

    with open('tweets_'+symbol+str(num)+'.json', 'w') as outfile:
        outfile.write(json.dumps(json_response, indent=4, sort_keys=True))

    if 'next' in json_response:
      query_params['next'] = json_response['next']

    num += 1

"""# Read entire JSON files to memory (List of dictionaries)"""

from glob import glob

tweet_json = []

for filename in glob('tweets_'+symbol+'*.json'):
  print(filename)
  with open(filename, 'rb') as readfile:
    tweet_json.append(json.load(readfile))

"""# Extract specific fields to memory (List of dictionaries) """

tweets_dict = []
single_tweet = {}

for file in tweet_json:
  for t in file['results']:
    if t['text'][0:2] != "RT" and t['retweeted'] == False:
      single_tweet['id_str'] = t['id_str']
      single_tweet['created_at'] = t['created_at']
      single_tweet['place'] = t['place']
      single_tweet['source'] = t['source']
      single_tweet['text'] = t['text']
      single_tweet['lang'] = t['lang']
      if t['truncated'] == True:
        single_tweet['text'] = t['extended_tweet']['full_text']
      single_tweet['coordinates'] = t['coordinates']
      single_tweet['geo'] = t['geo']
      single_tweet['coordinates'] = t['coordinates']
      single_tweet['is_quote_status'] = t['is_quote_status']
      single_tweet['retweet_count'] = t['retweet_count']
      single_tweet['quote_count'] = t['quote_count']
      single_tweet['reply_count'] = t['reply_count']

      single_tweet['user_'+'created_at'] = t['user']['created_at']
      single_tweet['user_'+'description'] = t['user']['description']
      single_tweet['user_'+'followers_count'] = t['user']['followers_count']
      single_tweet['user_'+'following'] = t['user']['following']
      single_tweet['user_'+'friends_count'] = t['user']['friends_count']
      single_tweet['user_'+'id_str'] = t['user']['id_str']
      single_tweet['user_'+'location'] = t['user']['location']
      single_tweet['user_'+'name'] = t['user']['name']
      single_tweet['user_'+'statuses_count'] = t['user']['statuses_count']
      single_tweet['user_'+'created_at'] = t['user']['created_at']
      single_tweet['user_'+'created_at'] = t['user']['created_at']

      tweets_dict.append(single_tweet)
      single_tweet = {}

tweets_dict

"""# Create dataframe"""

df = pd.DataFrame(tweets_dict)

df.describe()

data_table.DataTable(df, include_index=False, num_rows_per_page=10,max_columns=30)

"""# Save dataframe to CSV File"""

df.to_csv("tweets_"+symbol+"_raw.csv",index=False)