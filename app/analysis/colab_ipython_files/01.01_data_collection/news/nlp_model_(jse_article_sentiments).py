# -*- coding: utf-8 -*-
"""NLP Model (JSE Article Sentiments).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I_qb4BTb3XrFaRBG8xOnxl9dHry6EHUr

## Preliminary items

References: 
https://github.com/peanutshawny/lstm-stock-predictor
"""

import re
import string
import unicodedata

# importing
import numpy as np
import pandas as pd
import requests
import spacy
import nltk
nltk.download('stopwords')
nltk.download('vader_lexicon')

nlp = spacy.load('en_core_web_sm')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords

pd.set_option('max_colwidth', 400)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext google.colab.data_table

"""## Mount Google Drive """

# Commented out IPython magic to ensure Python compatibility.
currentWorkingDir = !pwd
defaultWorkingDir = "/content"

if ( currentWorkingDir[0] == defaultWorkingDir ):
  from google.colab import drive

  drive.mount('/content/drive')
      
#   %cd "/content/drive/My Drive/Colab Notebooks/stock_portfolio"
else:
  print("Currenting running app from: ")
  !pwd

"""# Import articles to dataframe"""

df = pd.read_csv("articles.csv")

df.content = df.content.astype('str')

df.dtypes

df.head(10)

"""# Clean up data

## Remove articles outside of scope
"""

df_sentiment = df

df_sentiment.post_date.min()

df_sentiment.post_date.max()

df_sentiment.post_date = pd.to_datetime(df_sentiment.post_date, format="%Y-%m-%d").dt.date
df_sentiment.post_date = pd.to_datetime(df_sentiment.post_date)

from datetime import datetime
df_sentiment = df_sentiment [(df_sentiment.post_date >= datetime(2016,1,1)) & (df_sentiment.post_date < datetime(2021,1,1)) ]

df_sentiment.shape

df_sentiment.post_date

"""## Remove fields that have no content"""

df_sentiment = df_sentiment[df_sentiment.content_length > 0]

"""## Clean up content field (remove "click here")

Filter for keywords
"""

df_sentiment[df_sentiment.content.str.contains("|".join(["click here"]),case=False) ]

"""Use regex to remove phrases"""

import re

p = re.compile("(Click here to open \\bdocuments?)|(Click here to (download|open))|(Click here to view \\bdocuments?)|(Click here)|(Please click here to download)",re.IGNORECASE)

df_sentiment["content_clean"] = df_sentiment.content.str.replace(p,"",regex=True)

df_sentiment[df_sentiment.content.str.contains("|".join(["click here"]),case=False) ]

"""Get content_clean length"""

df_sentiment['content_clean_length'] = df_sentiment.content_clean.str.len()

filter_click_here = df_sentiment.content.str.contains("|".join(["click here"]),case=False)

df_sentiment[filter_click_here]

"""Replace fields that contain "click here" with text from URL"""

df_sentiment['new_content'] = np.where(filter_click_here, df_sentiment.link,df_sentiment.content_clean)

d = df_sentiment.link.str.replace("https://www.jamstockex.com/", "")

e = d.str.replace("-"," ")

link_txt = e.str.replace("/","")

df_sentiment['content'] = np.where(filter_click_here,link_txt ,df_sentiment.content_clean)

df_sentiment['content_length'] = df_sentiment.content.str.len()

df_sentiment[ df_sentiment.content_length <= 7]

"""Remove the fields with blank values"""

column_list = ["postid","post_date","instrument_code","title","link","content"] 
filter_content_len = ~(df_sentiment.content_length <= 7)

df_sentiment = df_sentiment.loc[ filter_content_len, column_list ]

df_sentiment

"""# Create fields based on nature of article

## Create field for articles that pertain to financial reports
"""

df_sentiment[df_sentiment.content.str.contains("|".join(["quarter","q\d","qtr","audited","report","results","statements","supplementary"]),case=False)]

filter_report = df_sentiment.content.str.contains("|".join(["quarter","q\d","qtr","audited","report","results","statements","supplementary"]),case=False)

df_sentiment['is_regarding_financial_report'] = np.where(filter_report,1,0)

df_sentiment

"""## Create field based on when shares are bought/sold by connected parties"""

filter_shares = df_sentiment.content.str.contains("|".join(["sold|purchased"]),case=False) &  df_sentiment.content.str.contains("|".join(["shares"]),case=False)

df_sentiment[df_sentiment.content.str.contains("|".join(["sold|purchased"]),case=False) &  df_sentiment.content.str.contains("|".join(["shares"]),case=False)]

df_sentiment['is_sold_pur_shares'] = np.where(filter_shares,1,0)

df_sentiment

"""# Function to perform sentiment analysis"""

def getSentiment(article):
    """
    function that generates a soup to process text and output sentiment scores
    """

    # empty list for sentiment data
    sentiment_list = np.empty(shape=0, dtype=object)

    # cleaning text by removing punctuation and stopwords, as well as lemmatization
    punctuations = string.punctuation
    sw = stopwords.words('english')

    # converting to unicode
    section = unicodedata.normalize('NFKD', article)
    section = section.replace('\t', ' ').replace('\n', '').replace('/s', '').replace('\'', '')

    # joining, removing unecessary characters, and truncating text
    text = ''.join(section)
    text = re.sub('\s+', ' ', text).strip()
    text = text[:40000]

    # creating spacy nlp variable to tokenize and remove punctuation
    doc = nlp(text)

    doc = [token.lemma_.lower().strip() for token in doc]
    doc = [token for token in doc if token.isalpha()]
    doc = [token for token in doc if token not in punctuations and token not in sw]

    # joining text and getting sentiment
    doc = ' '.join(doc)

    analyzer = SentimentIntensityAnalyzer()
    sentiment = analyzer.polarity_scores(doc)

    sentiment_list = np.append(sentiment_list, sentiment)

    # transposing each type of sentiment (pos, neg, neu) into separate features
    sentiment_df = pd.DataFrame({'sentiment': sentiment_list})

    return sentiment_df

"""# Add column with article sentiment"""

a = df_sentiment['content'].to_list()

sentimentsList = [getSentiment(x).values[0][0]['compound'] for x in a]

df_sentiment['sentiment'] = sentimentsList

df_sentiment

"""# Group sentiment information based on date and instrument_code"""

df_sentiment_ml = df_sentiment.groupby(["instrument_code","post_date"]).agg({"sentiment":"mean",
                                                                             "is_regarding_financial_report":"min",
                                                                             "is_sold_pur_shares":"min"})

df_sentiment_ml

"""# Save to CSV"""

df_sentiment.to_csv("articles_sentiment_processed.csv")
df_sentiment_ml.to_csv("articles_sentiment_processed_ML.csv")

"""# Combine sentiments with prices dataset

## Import into dataframes
"""

sen_df = pd.read_csv("articles_sentiment_processed_ML.csv")
prices_df = pd.read_csv("jse_main_price_2016-2020.csv")

prices_df = prices_df[["Symbol","Date","Close_Price","Today_High","Today_Low","Volume_non_block"]]

sen_df

prices_df

"""## Data Preparation: Prices dataframe

###Convert dates from string to datetime to facilite date calculations
"""

prices_df.Date   = pd.to_datetime(prices_df.Date,   format="%Y-%m-%d").dt.date
sen_df.post_date = pd.to_datetime(sen_df.post_date, format="%Y-%m-%d").dt.date

"""### Field: Previous business day"""

import datetime
from pandas.tseries.offsets import BDay
# BDay is business day

prices_df["Previous_business_day"] = prices_df.Date.apply(lambda x:(x-pd.tseries.offsets.BDay(1)))
prices_df.Previous_business_day = pd.to_datetime(prices_df.Previous_business_day, format="%Y-%m-%d").dt.date

prices_df

"""## Data Preparation: Prices dataframe (per Symbol)"""

### Function: Used for field "stock_not_traded_date_list"
# Function creates an array of dates that the stock was not traded on

date_lookback_limit = 5 # Determines max dates to lookup
def getDatesToSumSentiments (x):
  
  lb = min(date_lookback_limit, x['no_days_not_traded_since_last_traded'] )
  date_list = list()
  for i in range(1,lb+1):
    date_list.append(
      x['Date'] - pd.tseries.offsets.BDay(i)
    )
  return date_list

# Single df with all data
df_list = list()

for s in prices_df.Symbol.unique():
	
  #### Create prices df for each symbol and sort by Date field
  p_df = prices_df[prices_df.Symbol == s]
  p_df = p_df.sort_values(by='Date', ascending=True)

  #### Create sentiment df for each symbol and sort by Date field 
  s_df = sen_df[sen_df.instrument_code == s]
  s_df = s_df.sort_values(by='post_date', ascending=True)	

  ### Field: Determine if traded on previous business day 
  p_df["is_traded_on_Previous_business_day"] = \
    (p_df.Date.shift() == p_df.Previous_business_day )
    
    
  ### Field: Calculate the previous day last traded
  temp 		 				            = p_df.Date.shift()
  temp.iloc[0] 			        	= p_df.Date[p_df.Date.first_valid_index()]
  p_df["Previous_trade_day"]  = temp


  ### Calculate the number of days that the stock did not trade for prior 
  ### to the current date

  # Function to determine no of business days between dates inclusive
  f = lambda x: (len(pd.bdate_range(x['Previous_trade_day'], x['Date'] )))

  p_df['no_days_not_traded_since_last_traded'] = (p_df.apply(f, axis=1))

  p_df['no_days_not_traded_since_last_traded'] =  \
    p_df['no_days_not_traded_since_last_traded'] - 2 # All values were off by 2 days

  # Set the first date as zero as there is no data for it to check against
  p_df.no_days_not_traded_since_last_traded.iloc[0] = 0 


  ## Field: Determine the dates that the stock did not trade for
  p_df['stock_not_traded_date_list'] = p_df.apply(getDatesToSumSentiments, axis=1)

  ### Convert prices dataframe to 1NF by creating a separate row for each value in
  ### 'stock_not_traded_date_list' field

  p_df = p_df.explode('stock_not_traded_date_list')
    
  ### Convert date fields to string to faciliate join
  p_df.stock_not_traded_date_list = p_df.stock_not_traded_date_list.astype(str)
  p_df.Date = p_df.Date.astype(str)
  s_df.post_date = s_df.post_date.astype(str)

  ### Perform join
  c_df = pd.merge(p_df, s_df, how='left', left_on="stock_not_traded_date_list",right_on="post_date")
  c_df = pd.merge(c_df, s_df, how='left', left_on="Date",right_on="post_date")


  # Group columns
  c_df = c_df.groupby(["Symbol","Date"]).agg({'Close_Price'     :'mean',
                                              'Volume_non_block':'mean',
                                              'no_days_not_traded_since_last_traded':'mean',
                                              'Today_High'      :'mean',
                                              'Today_Low'       :'mean',
                                              'sentiment_x'     :'mean',
                                              'is_regarding_financial_report_x' :'max',
                                              'is_sold_pur_shares_x'            :'max',
                                              'sentiment_y'     :'mean',
                                              'is_regarding_financial_report_y' :'max',
                                              'is_sold_pur_shares_y'            :'max'
                                              })
  
  c_df['sentiment'] =  c_df[["sentiment_x","sentiment_y"]].mean(axis=1)
  c_df['is_regarding_financial_report'] = c_df[["is_regarding_financial_report_x", "is_regarding_financial_report_y"]].max(axis=1)
  c_df['is_sold_pur_shares'] = c_df[["is_sold_pur_shares_x", "is_sold_pur_shares_y"]].max(axis=1)

  c_df = c_df[[
               "Close_Price", 
               "Volume_non_block",
               "Today_High",
               "Today_Low",
               "no_days_not_traded_since_last_traded",
               "sentiment",
               "is_regarding_financial_report",
               "is_sold_pur_shares"]]

  # Save individually to file then to list for saving to single file
  c_df.to_csv("prices_sentiment_"+s+".csv")
  df_list.append(c_df)

# Combine then save all to file
dfs = pd.concat(df_list)
dfs.to_csv("prices_sentiment__all.csv")

dfs.corr()['Close_Price']























"""### Field: Determine if traded on previous business day"""

prices_df["is_traded_on_Previous_business_day"] = (ccc_df.Date.shift() == ccc_df.Previous_business_day )

prices_df

"""### Field: Calculate the previous day last traded"""

temp = ccc_df.Date.shift()
temp.iloc[0] = ccc_df.Date[ccc_df.Date.first_valid_index()]

ccc_df["Previous_trade_day"] = temp

"""### Calculate the number of days that the stock did not trade for prior to the current date"""

f = lambda x: (len(pd.bdate_range(x['Previous_trade_day'], x['Date'] )))

ccc_df['no_days_not_traded_since_last_traded'] = (ccc_df.apply(f, axis=1))

ccc_df['no_days_not_traded_since_last_traded'] = ccc_df['no_days_not_traded_since_last_traded'] -2
ccc_df.no_days_not_traded_since_last_traded.iloc[0] = 0 # Set the first date as zero as there is no data for it to check against

ccc_df[ccc_df.is_traded_on_Previous_business_day == False]

"""## Determine the dates that the stock did not trade for"""

def getDatesToSumSentiments (x):
  date_list = list()
  for i in range(x['no_days_not_traded_since_last_traded']):
    date_list.append(
        x['Date'] - pd.tseries.offsets.BDay(i)
    )
  return date_list

ccc_df['stock_not_traded_date_list'] = ccc_df.apply(getDatesToSumSentiments, axis=1)

ccc_df

"""### Convert prices dataframe to 1NF by creating a separate row for each value in 'stock_not_traded_date_list'"""

ccc_df_exp = ccc_df.explode('stock_not_traded_date_list')

"""### Convert fields to string to faciliate join"""

ccc_df_exp.stock_not_traded_date_list = ccc_df_exp.stock_not_traded_date_list.dt.strftime("%Y-%m-%d")

sen_df.post_date = sen_df.post_date.dt.strftime("%Y-%m-%d")

"""### Perform join"""

ccc_com = pd.merge(  ccc_df_exp[["Symbol","Date","Close_Price","stock_not_traded_date_list"]],
                          ccc_sen_df[["post_date","sentiment","is_regarding_financial_report","is_sold_pur_shares"]], 
                          how='left',
                          left_on="stock_not_traded_date_list",right_on="post_date")

ccc_com

"""## Combine price df and sentiment df"""

ccc_combined = pd.merge(  ccc_df[["Symbol","Date","Close_Price"]],
                          ccc_sen_df[["post_date","sentiment","is_regarding_financial_report","is_sold_pur_shares"]], 
                          how='left',
                          left_on="Date",right_on="post_date")

ccc_combined[~(ccc_combined.sentiment.isna())].drop(columns=["post_date"])

